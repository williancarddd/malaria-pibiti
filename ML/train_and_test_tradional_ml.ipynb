{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.csv', '2.csv', '3.csv', '4.csv', '5.csv', '6.csv', '7.csv', '8.csv', '9.csv', '10.csv', '11.csv', '12.csv', '13.csv', '14.csv', '15.csv', '16.csv', '17.csv', '18.csv', '19.csv', '20.csv', '21.csv', '22.csv', '23.csv', '24.csv', '25.csv', '26.csv', '27.csv', '28.csv', '29.csv', '30.csv', '31.csv', '32.csv', '33.csv', '34.csv', '35.csv', '36.csv', '37.csv', '38.csv', '39.csv', '40.csv', '41.csv', '42.csv', '43.csv', '44.csv', '45.csv', '46.csv', '47.csv', '48.csv', '49.csv', '50.csv', '51.csv', '52.csv', '53.csv', '54.csv', '55.csv', '56.csv', '57.csv', '58.csv', '59.csv', '60.csv', '61.csv', '62.csv', '63.csv', '64.csv', '65.csv', '66.csv', '67.csv', '68.csv', '69.csv', '70.csv', '71.csv', '72.csv', '73.csv', '74.csv', '75.csv', '76.csv', '77.csv', '78.csv', '79.csv', '80.csv', '81.csv', '82.csv', '83.csv', '84.csv', '85.csv', '86.csv', '87.csv', '88.csv', '89.csv', '90.csv', '91.csv', '92.csv', '93.csv', '94.csv', '95.csv', '96.csv', '97.csv', '98.csv', '99.csv', '100.csv']\n"
     ]
    }
   ],
   "source": [
    "# Load partitions path: /media/william/NVME/projects/malaria-pibiti/1_entrada/partitions\n",
    "path_partitions = '/media/william/NVME/projects/malaria-pibiti/1_entrada/partitions'\n",
    "partitions = os.listdir(path_partitions) # 100 partitions Image,Class,Train,Test , 1.csv 2.csv 3.csv ... 100.csv\n",
    "# order by partition number\n",
    "partitions.sort(key=lambda x: int(x.split('.')[0]))\n",
    "print(partitions)\n",
    "\n",
    "# Path to Image in partition\n",
    "path_image = '/media/william/NVME/projects/malaria-pibiti/1_entrada/'\n",
    "dataset_names = [  'Dataset01_100', 'Dataset01_95.0',  'Dataset01_90.0']\n",
    "\"\"\" 'Dataset01_85.0' , 'Dataset01_80.0',\n",
    "                          'Dataset01_75.0',  'Dataset01_70.0',  'Dataset01_65.0',  'Dataset01_60.0',  'Dataset01_55.0',\n",
    "                          'Dataset01_50.0',  'Dataset01_45.0',  'Dataset01_40.0','Dataset01_35.0',  'Dataset01_30.0',\n",
    "                            'Dataset01_25.0',  'Dataset01_20.0',  'Dataset01_15.0',  'Dataset01_10.0',  'Dataset01_5.0'\"\"\"\n",
    "\n",
    "# {'Dataset01__100': path_image+dataset_names[0]}\n",
    "paths_datasets = {dataset_names[i]: path_image+dataset_names[i] for i in range(len(dataset_names))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results\n",
    "\n",
    "path_results = '/media/william/NVME/projects/malaria-pibiti/6_resultados'\n",
    "\n",
    "def make_results_folders(path_results, dataset_name, method):\n",
    "    path_dataset = os.path.join(path_results, dataset_name)\n",
    "    if not os.path.exists(path_dataset):\n",
    "        os.mkdir(path_dataset)\n",
    "    path_method = os.path.join(path_dataset, method)\n",
    "    if not os.path.exists(path_method):\n",
    "        os.mkdir(path_method)\n",
    "    path_metrics = os.path.join(path_method, 'metrics')\n",
    "    if not os.path.exists(path_metrics):\n",
    "        os.mkdir(path_metrics)\n",
    "    path_csvs = os.path.join(path_method, 'csvs')\n",
    "    if not os.path.exists(path_csvs):\n",
    "        os.mkdir(path_csvs)\n",
    "    path_test = os.path.join(path_method, 'test')\n",
    "    if not os.path.exists(path_test):\n",
    "        os.mkdir(path_test)\n",
    "    return path_metrics, path_csvs, path_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config video \n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "methodsNames = [ 'KNN'] #[,'NBayes',  'RandomForest', 'NBayes'] \n",
    "input_size = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import NuSVC, SVC, LinearSVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from tune_sklearn import TuneGridSearchCV\n",
    "\n",
    "def train_ml_algorithm(X_train, y_train, methodName):\n",
    "\n",
    "    search = None\n",
    "    # KNN\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "    if (methodName == 'KNN'): \n",
    "        parameters = {\n",
    "            \"n_neighbors\" : [1, 3, 5, 10, 15, 20],\n",
    "            \"weights\": ['uniform', 'distance'],\n",
    "            \"algorithm\": ['ball_tree', 'kd_tree', 'brute'],\n",
    "            \"leaf_size\": [5, 15, 25, 35, 45, 55],\n",
    "            'p': [10, 20, 40],\n",
    "            \"metric\": ['euclidean', 'manhattan', 'chebyshev', 'minkowski'],\n",
    "            # 'metric_params': [None],\n",
    "            'n_jobs': [3]\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=KNeighborsClassifier(),\n",
    "            param_grid=parameters,\n",
    "            verbose=1,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    # Decision Tree\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "    elif (methodName == 'DTree'): \n",
    "        parameters = {\n",
    "            'criterion': [\"gini\", \"entropy\"],\n",
    "            'splitter': ['best','random'],\n",
    "            # 'max_depth': [None],\n",
    "            # 'min_samples_split': [None],\n",
    "            # 'min_samples_leaf': [None],\n",
    "            # 'min_weight_fraction_leaf': [None],\n",
    "            'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            # 'random_state': [None],\n",
    "            # 'max_leaf_nodes': [None],\n",
    "            # 'min_impurity_decrease': [None],\n",
    "            'class_weight': [None, 'balanced'],\n",
    "            # 'ccp_alpha': [None]\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=DecisionTreeClassifier(),\n",
    "            param_grid=parameters,\n",
    "            scoring='accuracy',\n",
    "            # use_gpu=True,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            verbose=1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "    \n",
    "    # SVM Linear\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "    elif (methodName == \"SVMLinear\"):  \n",
    "        parameters = {\n",
    "            'penalty': ['l1','l2'],\n",
    "            # 'loss': ['hinge', 'squared_hinge'],\n",
    "            'dual': [True, False],\n",
    "            # 'tol': [1],\n",
    "            'C': np.arange(0.01,100,10),\n",
    "            'multi_class': ['ovr', 'crammer_singer'],\n",
    "            'fit_intercept': [True, False],\n",
    "            # 'intercept_scaling': [None],\n",
    "            'class_weight': [None, 'balanced'],\n",
    "            'verbose': [0],\n",
    "            # 'random_state': [None, 1, 3, 5, 7],\n",
    "            'max_iter': [1, 10, 50, 100, 200]\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=LinearSVC(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    # SVM Nu\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC\n",
    "    elif (methodName == 'SVMNu'):  \n",
    "        parameters = {\n",
    "            'nu': np.arange(0.1,1.1,0.1),\n",
    "            'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], # 'precomputed' requires an square matrix\n",
    "            'degree': [1, 2, 3, 4, 5],\n",
    "            'gamma': ['scale', 'auto'],\n",
    "            # 'coef0': [None],\n",
    "            'shrinking': [True, False],\n",
    "            'probability': [True],\n",
    "            # 'tol': [None],\n",
    "            # 'cache_size': [None],\n",
    "            'class_weight': [None, 'balanced'],\n",
    "            'verbose': [0],\n",
    "            'max_iter': [-1],\n",
    "            'decision_function_shape': ['ovo', 'ovr'],\n",
    "            'break_ties': [True, False],\n",
    "            # 'random_state': [None],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=NuSVC(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    # SVM C\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "    elif (methodName == 'SVMC'):  \n",
    "        parameters = {\n",
    "            'C': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1],\n",
    "            'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'degree': [1, 2, 3, 4, 5],\n",
    "            'gamma': ['scale', 'auto'],\n",
    "            # 'coef0': [1, 2, 3],\n",
    "            'shrinking': [True, False],\n",
    "            'probability': [True, False],\n",
    "            # 'tol': [None],\n",
    "            # 'cache_size': [None],\n",
    "            'class_weight': [None, 'balanced'],\n",
    "            'verbose': [0],\n",
    "            'max_iter': [-1],\n",
    "            'probability':[True],\n",
    "            'decision_function_shape': ['ovo', 'ovr'],\n",
    "            # 'break_ties': [True, False],\n",
    "            # 'random_state': [None],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=SVC(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    \n",
    "\n",
    "    # Discriminant Analysis (Linear)\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
    "    elif (methodName == 'DAnalysisLinear'): \n",
    "        parameters = {\n",
    "            'solver': ['svd', 'lsqr', 'eigen'],\n",
    "            'shrinkage': ['auto', 0.2, 0.4, 0.6,0.8, 1],\n",
    "            # 'priors': [None],\n",
    "            # 'n_components': [None, 10, 20, 40, 50, 100, 200],\n",
    "            'store_covariance': [True, False],\n",
    "            # 'tol': [None],\n",
    "            # 'covariance_estimator': [None],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=LinearDiscriminantAnalysis(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    # SGD\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html?highlight=sgd#sklearn.linear_model.SGDClassifier\n",
    "    elif (methodName == 'SGD'): \n",
    "        parameters = {\n",
    "            # 'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "            'epsilon':[0.01, 0.1, 1]\n",
    "            # 'l1_ratio': [0.10, 0.15, 0.2, 0.25],\n",
    "            # 'fit_intercept': [True, False],\n",
    "            # 'max_iter': [1000, 1200,1400],\n",
    "            # 'shuffle': [True, False],\n",
    "            # 'early_stopping': [True],\n",
    "            # 'n_iter_no_change': [3],\n",
    "        }\n",
    "        lr = SGDClassifier(loss='hinge')\n",
    "        search  = TuneGridSearchCV( \n",
    "            estimator=lr,\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    # Discriminant Analysis (Quadratic)\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis\n",
    "    elif (methodName == 'DAnalysisQuadratic'): \n",
    "        parameters = {\n",
    "            # 'priors': [None],\n",
    "            'reg_param': [0.1, 0.2, 0.3],\n",
    "            'store_covariance': [True, False],\n",
    "            # 'tol': [None],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=QuadraticDiscriminantAnalysis(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    # Naive Bayes\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB\n",
    "    elif (methodName == 'NBayes'): \n",
    "        parameters = {\n",
    "            # 'priors': None,\n",
    "            \"var_smoothing\" : np.logspace(0,-9, num=100),\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=GaussianNB(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "\n",
    "    # AdaBoost\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html?highlight=adaboost#sklearn.ensemble.AdaBoostClassifier\n",
    "    elif (methodName == 'AdaBoost'): \n",
    "        parameters = {\n",
    "            # 'base_estimator': [None],\n",
    "            \"n_estimators\" : [10, 50, 100, 150],\n",
    "            'learning_rate': [0.5, 1, 1.5],\n",
    "            'algorithm': ['SAMME', 'SAMME.R'],\n",
    "            # 'random_state': [3,5,7,9],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=AdaBoostClassifier(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    # RandomForest\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=random#sklearn.ensemble.RandomForestClassifier\n",
    "    elif (methodName == 'RandomForest'): \n",
    "        parameters = {\n",
    "            \"n_estimators\" : [10, 100, 1000],\n",
    "            'criterion': [\"gini\", \"entropy\"],\n",
    "            'max_depth': [None],\n",
    "            \"max_features\" : ['sqrt', 'log2'],\n",
    "            'verbose': [0],\n",
    "            'class_weight': ['balanced','balanced_subsample'],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=RandomForestClassifier(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    # Bagging\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
    "    elif (methodName == 'Bagging'): \n",
    "        parameters = {\n",
    "            \"n_estimators\" : [10, 50, 100, 150, 100],\n",
    "            'verbose': [0],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=BaggingClassifier(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    # ExtraTreesClassifier\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n",
    "    elif (methodName == 'ExtraTrees'): \n",
    "        parameters = {\n",
    "            \"n_estimators\" : [10, 50, 100, 150, 100],\n",
    "            'criterion': [\"gini\", \"entropy\"],\n",
    "            \"max_features\" : ['sqrt', 'log2'],\n",
    "            'class_weight': ['balanced', 'balanced_subsample'],\n",
    "            'verbose': [0],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=ExtraTreesClassifier(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    # GradientBoostingClassifier\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "    elif (methodName == 'GradientBoosting'): \n",
    "        parameters = {\n",
    "            'loss': ['deviance', 'exponential'],\n",
    "            \"n_estimators\" : [10, 50, 100, 150, 100],\n",
    "            'criterion': [\"squared_error\" ],\n",
    "            \"max_features\" : ['sqrt', 'log2'],\n",
    "            'verbose': [0],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=GradientBoostingClassifier(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    # StackingClassifier\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier\n",
    "    elif (methodName == 'Stacking'): \n",
    "        parameters = {\n",
    "            'stack_method': ['predict_proba', 'decision_function', 'predict'],\n",
    "            \"passthrough\" : [True, False],\n",
    "            'verbose': [0],\n",
    "        }\n",
    "        clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "        clf2 = RandomForestClassifier(random_state=1)\n",
    "        clf3 = GaussianNB()\n",
    "        lr = LogisticRegression()\n",
    "        search  = GridSearchCV( \n",
    "            estimator=StackingClassifier(estimators=[clf1, clf2, clf3],  \n",
    "                          final_estimator=lr),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    # VotingClassifier\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier\n",
    "    elif (methodName == 'Voting'): \n",
    "        parameters = {\n",
    "            'voting': ['hard', 'soft'],\n",
    "            \"flatten_transform\" : [True, False],\n",
    "            'verbose': [0],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=VotingClassifier(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    # HistGradientBoostingClassifier\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier\n",
    "    elif (methodName == 'HistGradient'): \n",
    "        parameters = {\n",
    "            'loss': ['binary_crossentropy', 'categorical_crossentropy'],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=HistGradientBoostingClassifier(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        results = None\n",
    "    \n",
    "    \n",
    "    if (search != None):\n",
    "        results = search.fit(X_train, y_train)\n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure colors\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeTrain = 0\n",
    "runtimeTest = 0\n",
    "\n",
    "# Configure personal metrics\n",
    "def specificity(tn, fp):\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "# Negative Predictive Error\n",
    "def npv(tn, fn):\n",
    "    return tn / (tn + fn + 1e-7)\n",
    "\n",
    "# Matthews Correlation_Coefficient\n",
    "def mcc(tp, tn, fp, fn):\n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    return num / np.sqrt(den + 1e-7)\n",
    "\n",
    "\n",
    "def calculateMeasures(Y_pred, Y_true, Yscores, y_pred, y_true, yscores, folder, methodName, thresh, save_metrics_path):\n",
    "    metrics = pd.DataFrame()\n",
    "    tn, fp, fn, tp = confusion_matrix(Y_true, Y_pred, labels=[0,1]).ravel()\n",
    "    #fpr, tpr, _ = roc_curve(y_true, scores, pos_label=2)\n",
    "    auc_val = roc_auc_score(Y_true, Yscores)\n",
    "\n",
    "    metrics['folder'] = [folder]\n",
    "    metrics['network'] = [methodName]\n",
    "\n",
    "    # Train RESULTS\n",
    "    metrics['accuracy'] = [accuracy_score(Y_true, Y_pred)]\n",
    "    metrics['precision'] = [precision_score(Y_true, Y_pred)]\n",
    "    metrics['sensitivity'] = [recall_score(Y_true, Y_pred)]\n",
    "    metrics['specificity'] = [specificity(tn,fp)]\n",
    "    metrics['fmeasure'] = [f1_score(Y_true, Y_pred)]\n",
    "    metrics['npv'] = [npv(tn, fn)]\n",
    "    metrics['mcc'] = [mcc(tp, tn, fp, fn)]\n",
    "    metrics['auc'] = [auc_val]\n",
    "    metrics['tn'] = [tn]\n",
    "    metrics['fp'] = [fp]\n",
    "    metrics['fn'] = [fn]\n",
    "    metrics['tp'] = [tp]\n",
    "    metrics['runtime'] = [runtimeTrain]\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    #fpr, tpr, _ = roc_curve(y_true, scores, pos_label=2)\n",
    "    auc_val = roc_auc_score(y_true, yscores)\n",
    "\n",
    "    # Test RESULTS\n",
    "    metrics['val_accuracy'] = [accuracy_score(y_true, y_pred)]\n",
    "    metrics['val_precision'] = [precision_score(y_true, y_pred)]\n",
    "    metrics['val_sensitivity'] = [recall_score(y_true, y_pred)]\n",
    "    metrics['val_specificity'] = [specificity(tn,fp)]\n",
    "    metrics['val_fmeasure'] =[f1_score(y_true, y_pred)]\n",
    "    metrics['val_npv'] = [npv(tn, fn)]\n",
    "    metrics['val_mcc'] = [mcc(tp, tn, fp, fn)]\n",
    "    metrics['val_auc'] = [auc_val]\n",
    "    metrics['val_tn'] = [tn]\n",
    "    metrics['val_fp'] = [fp]\n",
    "    metrics['val_fn'] = [fn]\n",
    "    metrics['val_tp'] = [tp]\n",
    "    metrics['val_runtime'] = [runtimeTest]\n",
    "\n",
    "    print(bcolors.FAIL + 'ACC: %.2f' %(100*metrics['val_accuracy'][0]) + ' AUC: %.2f' %(100*metrics['val_auc'][0]) + bcolors.ENDC)\n",
    "\n",
    "    if os.path.exists(os.path.join(save_metrics_path, methodName + str(thresh*100) + '.csv')):\n",
    "        metrics.to_csv(os.path.join(save_metrics_path, methodName + str(thresh*100) + '.csv'), sep=',', mode='a', index=False, header=False)\n",
    "    else:\n",
    "        metrics.to_csv(os.path.join(save_metrics_path, methodName + str(thresh*100) + '.csv'), sep=',', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "def load_dataset(n_classes: int, path_partition_name: str, data_set_name: str) -> tuple:\n",
    "      TrainImages, TestImages, TrainLabels, TestLabels = list(), list(), list(), list()\n",
    "\n",
    "      \n",
    "      try:\n",
    "          with open(path_partition_name, 'r') as file:\n",
    "              csvreader = csv.reader(file)\n",
    "              next(csvreader)  # Skip header if present\n",
    "              for row in csvreader:\n",
    "                  if row[2] == 'True':   # Train sample\n",
    "                      TrainImages.append(row[0])\n",
    "                      TrainLabels.append(int(row[1]))\n",
    "                  elif row[3] == 'True': # Test sample\n",
    "                      TestImages.append(row[0])\n",
    "                      TestLabels.append(int(row[1]))\n",
    "\n",
    "          TrainImages = [select_image(paths_datasets[data_set_name] + '/' + img) for img in TrainImages]\n",
    "          TestImages = [select_image(paths_datasets[data_set_name] + '/' + img) for img in TestImages]\n",
    "          print(sum(TrainLabels))\n",
    "          print(sum(TestLabels))\n",
    "          TrainLabels = to_categorical(np.array(TrainLabels), num_classes=n_classes)\n",
    "          TestLabels = to_categorical(np.array(TestLabels), num_classes=n_classes)\n",
    "\n",
    "          return np.array(TrainImages)/255.0, TrainLabels, np.array(TestImages)/255.0, TestLabels\n",
    "\n",
    "      except Exception as e:\n",
    "          print(f\"Error processing dataset: {e}\")\n",
    "          return None, None, None, None\n",
    "      \n",
    "def select_image(path_img_filename):\n",
    "    \"\"\"\n",
    "    Select and preprocess a single image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = Image.open(path_img_filename)\n",
    "        image = np.asarray(image.convert('RGB'))\n",
    "        image = tf.image.resize_with_crop_or_pad(image, input_size[0], input_size[1])\n",
    "        image = tf.image.resize(image, [input_size[0], input_size[1]])\n",
    "        return np.asarray(image) \n",
    "    except IOError:\n",
    "        print(f\"Error opening {path_img_filename}.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN on partition 1.csv with dataset Dataset01_100...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2373\n",
      "628\n",
      "Fitting 1 folds for each of 2592 candidates, totalling 2592 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/.local/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid = os.fork()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ml_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m runtimeTrain \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mrefit_time_\n\u001b[1;32m     13\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "Cell \u001b[0;32mIn[26], line 365\u001b[0m, in \u001b[0;36mtrain_ml_algorithm\u001b[0;34m(X_train, y_train, methodName)\u001b[0m\n\u001b[1;32m    361\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (search \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 365\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for network in methodsNames:\n",
    "    for partition in partitions:\n",
    "        for dataset_name in dataset_names:\n",
    "            print(f\"Training {network} on partition {partition} with dataset {dataset_name}...\")\n",
    "            path_metrics, path_csvs, path_test = make_results_folders(path_results, dataset_name, network)\n",
    "            path_partition = os.path.join(path_partitions, partition)\n",
    "            X_train, y_train, X_test, y_test = load_dataset(2, path_partition, dataset_name)\n",
    "            if X_train is None:\n",
    "                continue\n",
    "            results = train_ml_algorithm(X_train, y_train, network)\n",
    "            runtimeTrain = results.refit_time_\n",
    "\n",
    "            Y_pred = results.predict(X_train)\n",
    "            Yscores = results.predict_proba(X_train)[:,1]\n",
    "            y_pred = results.predict(X_test)\n",
    "            yscores = results.predict_proba(X_test)[:,1]\n",
    "\n",
    "            calculateMeasures(Y_pred, y_train, Yscores, y_pred, y_test, yscores, partition, network, 0.2, path_metrics)\n",
    "            runtimeTest = results.score(X_test, y_test)\n",
    "            print(f\"Runtime: {runtimeTest}\")\n",
    "            print(f\"Finished training {network} on partition {partition} with dataset {dataset_name}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
