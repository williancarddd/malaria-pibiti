{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import logging\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='malaria_ml_features.log', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuração dos paths\n",
    "path_features = '/media/williancaddd/CODES/projects/malaria-pibiti/1_entrada'\n",
    "dataset_names = ['Dataset01_75.0']\n",
    "dataset_names = dataset_names[::-1]\n",
    "paths_datasets = {dataset_names[i]: os.path.join(path_features, dataset_names[i], \"features/features.csv\") for i in range(len(dataset_names))}\n",
    "path_results = '/media/williancaddd/CODES/projects/malaria-pibiti/6_resultados'\n",
    "\n",
    "# Nome dos métodos\n",
    "methodsNames = ['GradientBoosting'] # ['GradientBoosting', \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar diretórios para salvar resultados\n",
    "def make_results_folders(path_results, dataset_name, method):\n",
    "    path_dataset = os.path.join(path_results, dataset_name)\n",
    "    if not os.path.exists(path_dataset):\n",
    "        os.mkdir(path_dataset)\n",
    "    path_method = os.path.join(path_dataset, method)\n",
    "    if not os.path.exists(path_method):\n",
    "        os.mkdir(path_method)\n",
    "    path_metrics = os.path.join(path_method, 'metrics')\n",
    "    if not os.path.exists(path_metrics):\n",
    "        os.mkdir(path_metrics)\n",
    "    path_csvs = os.path.join(path_method, 'csvs')\n",
    "    if not os.path.exists(path_csvs):\n",
    "        os.mkdir(path_csvs)\n",
    "    path_test = os.path.join(path_method, 'test')\n",
    "    if not os.path.exists(path_test):\n",
    "        os.mkdir(path_test)\n",
    "    return path_metrics, path_csvs, path_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config video \n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import  StandardScaler\n",
    "\n",
    "# get dynamic cores\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "# use 1/2 of the cores\n",
    "cores = cores - 1\n",
    "\n",
    "def train_ml_algorithm(X_train, y_train, methodName):\n",
    "\n",
    "    search = None\n",
    "    # KNN\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "    if (methodName == 'KNN'): \n",
    "        standardized_data = StandardScaler()\n",
    "        var_filter = VarianceThreshold()\n",
    "        knn = KNeighborsClassifier()\n",
    "        pipe = Pipeline([('standardized_data', standardized_data),\n",
    "                        ('var_filter', var_filter),\n",
    "                        ('knn', knn)])\n",
    "        # parameters = {\n",
    "        #     \"n_neighbors\" : [1, 2, 3, 4, 5],\n",
    "        #     \"weights\": ['uniform', 'distance'],\n",
    "        #     \"algorithm\": ['ball_tree', 'kd_tree', 'brute'],\n",
    "        #     \"leaf_size\": [5, 15, 25, 35, 45, 55],\n",
    "        #     'p': [10, 20, 40],\n",
    "        #     \"metric\": ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n",
    "        # }\n",
    "        parameters = [{'knn__n_neighbors':[1,2,3,4,5], 'knn__algorithm':['brute'], 'knn__metric':['euclidean']}]\n",
    "        search  = GridSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=cores,\n",
    "            verbose=1\n",
    "        )\n",
    "    \n",
    "    # AdaBoost\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "    elif (methodName == 'AdaBoost'): \n",
    "        parameters = {\n",
    "            # 'base_estimator': [None],\n",
    "            \"n_estimators\" : [100],\n",
    "            'algorithm': ['SAMME'],\n",
    "            'random_state': [0],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=AdaBoostClassifier(),\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            param_grid=parameters,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "   \n",
    "    # GradientBoosting\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "    elif (methodName == 'GradientBoosting'): \n",
    "        parameters = {\n",
    "            'loss': ['deviance', 'exponential'],\n",
    "            \"n_estimators\" : [10, 50, 100, 150, 100],\n",
    "            'criterion': [\"squared_error\" ],\n",
    "            \"max_features\" : ['sqrt', 'log2'],\n",
    "            'verbose': [0],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=GradientBoostingClassifier(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    # Naive Bayes\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB\n",
    "    elif (methodName == 'NBayes'): \n",
    "        parameters = {\n",
    "            # 'priors': None,\n",
    "            \"var_smoothing\" : np.logspace(0,-9, num=100),\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=GaussianNB(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=cores\n",
    "        )\n",
    "\n",
    "    # RandomForest\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=random#sklearn.ensemble.RandomForestClassifier\n",
    "    elif (methodName == 'RandomForest'): \n",
    "        parameters = {\n",
    "            \"n_estimators\" : [10, 100, 1000],\n",
    "            'criterion': [\"entropy\"],\n",
    "            'max_depth': [None],\n",
    "            \"max_features\" : ['sqrt', 'log2'],\n",
    "            'verbose': [0],\n",
    "            'class_weight': ['balanced','balanced_subsample'],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=RandomForestClassifier(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=cores\n",
    "        )\n",
    "    else:\n",
    "        results = None\n",
    "    \n",
    "    \n",
    "    if (search != None):\n",
    "        results = search.fit(X_train, y_train)\n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure personal metrics\n",
    "def specificity(tn, fp):\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "# Negative Predictive Error\n",
    "def npv(tn, fn):\n",
    "    return tn / (tn + fn + 1e-7)\n",
    "\n",
    "# Matthews Correlation_Coefficient\n",
    "def mcc(tp, tn, fp, fn):\n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    return num / np.sqrt(den + 1e-7)\n",
    "\n",
    "\n",
    "def calculateMeasures(dataset, Y_pred, Y_true, Yscores, y_pred, y_true, yscores, folder, methodName, thresh, save_metrics_path, runtimeTrain, runtimeTest):\n",
    "    metrics = pd.DataFrame()\n",
    "    tn, fp, fn, tp = confusion_matrix(Y_true, Y_pred, labels=[0,1]).ravel()\n",
    "    #fpr, tpr, _ = roc_curve(y_true, scores, pos_label=2)\n",
    "    auc_val = roc_auc_score(Y_true, Yscores)\n",
    "\n",
    "    metrics['dataset'] = [dataset]\n",
    "    metrics['network'] = [methodName]\n",
    "    metrics['partition'] = [folder]\n",
    "\n",
    "    \n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    #fpr, tpr, _ = roc_curve(y_true, scores, pos_label=2)\n",
    "    auc_val = roc_auc_score(y_true, yscores)\n",
    "\n",
    "    # Train RESULTS\n",
    "    metrics['accuracy'] = [accuracy_score(Y_true, Y_pred)]\n",
    "    metrics['precision'] = [precision_score(Y_true, Y_pred)]\n",
    "    metrics['sensitivity'] = [recall_score(Y_true, Y_pred)]\n",
    "    metrics['specificity'] = [specificity(tn,fp)]\n",
    "    metrics['f1_score'] = [f1_score(Y_true, Y_pred)]\n",
    "    metrics['npv'] = [npv(tn, fn)]\n",
    "    metrics['mcc'] = [mcc(tp, tn, fp, fn)]\n",
    "    metrics['auc'] = [auc_val]\n",
    "    metrics['TP'] = [tp]\n",
    "    metrics['TN'] = [tn]\n",
    "    metrics['FP'] = [fp]\n",
    "    metrics['FN'] = [fn]\n",
    "    metrics['runtime'] = [runtimeTrain]\n",
    "\n",
    "    # Test RESULTS\n",
    "    metrics['val_accuracy'] = [accuracy_score(y_true, y_pred)]\n",
    "    metrics['val_precision'] = [precision_score(y_true, y_pred)]\n",
    "    metrics['val_sensitivity'] = [recall_score(y_true, y_pred)]\n",
    "    metrics['val_specificity'] = [specificity(tn,fp)]\n",
    "    metrics['val_f1_score'] =[f1_score(y_true, y_pred)]\n",
    "    metrics['val_npv'] = [npv(tn, fn)]\n",
    "    metrics['val_mcc'] = [mcc(tp, tn, fp, fn)]\n",
    "    metrics['val_auc'] = [auc_val]\n",
    "    metrics['val_TP'] = [tp]\n",
    "    metrics['val_TN'] = [tn]\n",
    "    metrics['val_FP'] = [fp]\n",
    "    metrics['val_FN'] = [fn]\n",
    "    metrics['val_runtime'] = [runtimeTest]\n",
    "\n",
    "\n",
    "\n",
    "    print(bcolors.FAIL + 'ACC: %.2f' %(100*metrics['val_accuracy'][0]) + ' AUC: %.2f' %(100*metrics['val_auc'][0]) + bcolors.ENDC)\n",
    "\n",
    "    if os.path.exists(os.path.join(save_metrics_path, methodName + '.csv')):\n",
    "        metrics.to_csv(os.path.join(save_metrics_path, methodName + '.csv'), sep=',', mode='a', index=False, header=False)\n",
    "    else:\n",
    "        metrics.to_csv(os.path.join(save_metrics_path, methodName + '.csv'), sep=',', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_dataset(csv_path: str) -> tuple:\n",
    "    features = []\n",
    "    \"\"\"_summary_\n",
    "    # feature header histogram_Mean,histogram_Standard Deviation,histogram_Energy,histogram_Skewness,histogram_Entropy,histogram_Smoothness,histogram_Kurtosis,glcm_Contrast,glcm_Dissimilarity,glcm_Homogeneity,glcm_Energy,glcm_Correlation,image\n",
    "    # image 6-66-53-0.bmp 0 is the label\n",
    "\n",
    "    For each features file, the last column is the image name with the label in your name\n",
    "    \"\"\"\n",
    "\n",
    "    pd_data = pd.read_csv(csv_path)\n",
    "\n",
    "    # Get the features\n",
    "    features = pd_data.iloc[:, :-1].values\n",
    " \n",
    "    # Get the label  (last column), which is the image name with the label in your name, apply lambda to get only the label\n",
    "    labels = pd_data.iloc[:, -1].apply(lambda x: int(x.split('-')[-1].split('.')[0])).values\n",
    "\n",
    "    # remove column image from features\n",
    "\n",
    "    features = np.delete(features, 0, axis=1)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "all_datasets = {}\n",
    "for dataset_name, path in paths_datasets.items():\n",
    "    X, y = load_dataset(path)\n",
    "    all_datasets[dataset_name] = (X, y)\n",
    "\n",
    "\n",
    "splited_datasets = {}\n",
    "for dataset_name, (X, y) in all_datasets.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    splited_datasets[dataset_name] = (X_train, X_test, y_train, y_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    for network in methodsNames:\n",
    "        for dataset_name, (X, y) in all_datasets.items():\n",
    "            # Calcular o número mínimo de amostras por classe\n",
    "            min_samples_per_class = np.min(np.bincount(y))\n",
    "            \n",
    "            # Definir n_splits como o menor valor entre 100 e min_samples_per_class\n",
    "            n_splits = min(100, min_samples_per_class)\n",
    "            \n",
    "            skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "            fold = 1\n",
    "            path_metrics, path_csvs, path_test = make_results_folders(path_results, dataset_name, network)\n",
    "\n",
    "            for train_index, test_index in skf.split(X, y):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "  \n",
    "                start = time.time()\n",
    "                results = train_ml_algorithm(X_train, y_train, network)\n",
    "                runtimeTrain = time.time() - start\n",
    "                \n",
    "                start = time.time()\n",
    "                y_pred = results.predict(X_test)\n",
    "                y_scores = results.predict_proba(X_test)[:, 1]\n",
    "                runtimeTest = time.time() - start\n",
    "             \n",
    "                calculateMeasures(\n",
    "                    dataset_name, \n",
    "                    results.predict(X_train), y_train, results.predict_proba(X_train)[:, 1], \n",
    "                    y_pred, y_test, y_scores, fold, \n",
    "                    network + \"f\", 0.5, \n",
    "                    path_metrics, runtimeTrain, runtimeTest\n",
    "                )\n",
    "                print(f\"Finished {network} on {dataset_name}, Fold {fold}/{n_splits}\")\n",
    "                \n",
    "                del results\n",
    "                gc.collect()\n",
    "                \n",
    "                fold += 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
