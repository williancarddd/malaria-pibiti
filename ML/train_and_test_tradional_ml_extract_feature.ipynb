{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import logging\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='malaria_ml_features.log', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuração dos paths\n",
    "path_features = '/media/williancarddd/NVME/projects/malaria-pibiti/1_entrada/'\n",
    "dataset_names = ['Dataset01_100', 'Dataset01_95.0', 'Dataset01_90.0', 'Dataset01_85.0', 'Dataset01_80.0',\n",
    "                 'Dataset01_75.0', 'Dataset01_70.0', 'Dataset01_65.0', 'Dataset01_60.0', 'Dataset01_55.0',\n",
    "                 'Dataset01_50.0', 'Dataset01_45.0', 'Dataset01_40.0', 'Dataset01_35.0', 'Dataset01_30.0',\n",
    "                 'Dataset01_25.0', 'Dataset01_20.0', 'Dataset01_15.0', 'Dataset01_10.0', 'Dataset01_5.0']\n",
    "dataset_names = dataset_names[::-1]\n",
    "paths_datasets = {dataset_names[i]: os.path.join(path_features, dataset_names[i], \"features/features.csv\") for i in range(len(dataset_names))}\n",
    "path_results = '/media/williancarddd/NVME/projects/malaria-pibiti/6_resultados'\n",
    "\n",
    "# Nome dos métodos\n",
    "methodsNames = ['GradientBoosting', 'KNN', 'NBayes', 'RandomForest']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar diretórios para salvar resultados\n",
    "def make_results_folders(path_results, dataset_name, method):\n",
    "    path_dataset = os.path.join(path_results, dataset_name)\n",
    "    if not os.path.exists(path_dataset):\n",
    "        os.mkdir(path_dataset)\n",
    "    path_method = os.path.join(path_dataset, method)\n",
    "    if not os.path.exists(path_method):\n",
    "        os.mkdir(path_method)\n",
    "    path_metrics = os.path.join(path_method, 'metrics')\n",
    "    if not os.path.exists(path_metrics):\n",
    "        os.mkdir(path_metrics)\n",
    "    path_csvs = os.path.join(path_method, 'csvs')\n",
    "    if not os.path.exists(path_csvs):\n",
    "        os.mkdir(path_csvs)\n",
    "    path_test = os.path.join(path_method, 'test')\n",
    "    if not os.path.exists(path_test):\n",
    "        os.mkdir(path_test)\n",
    "    return path_metrics, path_csvs, path_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config video \n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import  StandardScaler\n",
    "\n",
    "# get dynamic cores\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "# use 1/2 of the cores\n",
    "cores = cores - 1\n",
    "\n",
    "def train_ml_algorithm(X_train, y_train, methodName):\n",
    "\n",
    "    search = None\n",
    "    # KNN\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "    if (methodName == 'KNN'): \n",
    "        standardized_data = StandardScaler()\n",
    "        var_filter = VarianceThreshold()\n",
    "        knn = KNeighborsClassifier()\n",
    "        pipe = Pipeline([('standardized_data', standardized_data),\n",
    "                        ('var_filter', var_filter),\n",
    "                        ('knn', knn)])\n",
    "        # parameters = {\n",
    "        #     \"n_neighbors\" : [1, 2, 3, 4, 5],\n",
    "        #     \"weights\": ['uniform', 'distance'],\n",
    "        #     \"algorithm\": ['ball_tree', 'kd_tree', 'brute'],\n",
    "        #     \"leaf_size\": [5, 15, 25, 35, 45, 55],\n",
    "        #     'p': [10, 20, 40],\n",
    "        #     \"metric\": ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n",
    "        # }\n",
    "        parameters = [{'knn__n_neighbors':[1,2,3,4,5], 'knn__algorithm':['brute'], 'knn__metric':['euclidean']}]\n",
    "        search  = GridSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=cores,\n",
    "            verbose=1\n",
    "        )\n",
    "    \n",
    "    # AdaBoost\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "    elif (methodName == 'AdaBoost'): \n",
    "        parameters = {\n",
    "            # 'base_estimator': [None],\n",
    "            \"n_estimators\" : [100],\n",
    "            'algorithm': ['SAMME'],\n",
    "            'random_state': [0],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=AdaBoostClassifier(),\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            param_grid=parameters,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "   \n",
    "    # GradientBoosting\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "    elif (methodName == 'GradientBoosting'): \n",
    "        parameters = {\n",
    "            'loss': ['deviance', 'exponential'],\n",
    "            \"n_estimators\" : [10, 50, 100, 150, 100],\n",
    "            'criterion': [\"squared_error\" ],\n",
    "            \"max_features\" : ['sqrt', 'log2'],\n",
    "            'verbose': [0],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=GradientBoostingClassifier(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    # Naive Bayes\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB\n",
    "    elif (methodName == 'NBayes'): \n",
    "        parameters = {\n",
    "            # 'priors': None,\n",
    "            \"var_smoothing\" : np.logspace(0,-9, num=100),\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=GaussianNB(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=cores\n",
    "        )\n",
    "\n",
    "    # RandomForest\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=random#sklearn.ensemble.RandomForestClassifier\n",
    "    elif (methodName == 'RandomForest'): \n",
    "        parameters = {\n",
    "            \"n_estimators\" : [10, 100, 1000],\n",
    "            'criterion': [\"entropy\"],\n",
    "            'max_depth': [None],\n",
    "            \"max_features\" : ['sqrt', 'log2'],\n",
    "            'verbose': [0],\n",
    "            'class_weight': ['balanced','balanced_subsample'],\n",
    "        }\n",
    "        search  = GridSearchCV( \n",
    "            estimator=RandomForestClassifier(),\n",
    "            param_grid=parameters,\n",
    "            cv=ShuffleSplit(test_size=0.01, n_splits=1, random_state=0),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=cores\n",
    "        )\n",
    "    else:\n",
    "        results = None\n",
    "    \n",
    "    \n",
    "    if (search != None):\n",
    "        results = search.fit(X_train, y_train)\n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure personal metrics\n",
    "def specificity(tn, fp):\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "# Negative Predictive Error\n",
    "def npv(tn, fn):\n",
    "    return tn / (tn + fn + 1e-7)\n",
    "\n",
    "# Matthews Correlation_Coefficient\n",
    "def mcc(tp, tn, fp, fn):\n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    return num / np.sqrt(den + 1e-7)\n",
    "\n",
    "\n",
    "def calculateMeasures(dataset, Y_pred, Y_true, Yscores, y_pred, y_true, yscores, folder, methodName, thresh, save_metrics_path, runtimeTrain, runtimeTest):\n",
    "    metrics = pd.DataFrame()\n",
    "    tn, fp, fn, tp = confusion_matrix(Y_true, Y_pred, labels=[0,1]).ravel()\n",
    "    #fpr, tpr, _ = roc_curve(y_true, scores, pos_label=2)\n",
    "    auc_val = roc_auc_score(Y_true, Yscores)\n",
    "\n",
    "    metrics['dataset'] = [dataset]\n",
    "    metrics['network'] = [methodName]\n",
    "    metrics['partition'] = [folder]\n",
    "\n",
    "    \n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    #fpr, tpr, _ = roc_curve(y_true, scores, pos_label=2)\n",
    "    auc_val = roc_auc_score(y_true, yscores)\n",
    "\n",
    "    # Train RESULTS\n",
    "    metrics['accuracy'] = [accuracy_score(Y_true, Y_pred)]\n",
    "    metrics['precision'] = [precision_score(Y_true, Y_pred)]\n",
    "    metrics['sensitivity'] = [recall_score(Y_true, Y_pred)]\n",
    "    metrics['specificity'] = [specificity(tn,fp)]\n",
    "    metrics['f1_score'] = [f1_score(Y_true, Y_pred)]\n",
    "    metrics['npv'] = [npv(tn, fn)]\n",
    "    metrics['mcc'] = [mcc(tp, tn, fp, fn)]\n",
    "    metrics['auc'] = [auc_val]\n",
    "    metrics['TP'] = [tp]\n",
    "    metrics['TN'] = [tn]\n",
    "    metrics['FP'] = [fp]\n",
    "    metrics['FN'] = [fn]\n",
    "    metrics['runtime'] = [runtimeTrain]\n",
    "\n",
    "    # Test RESULTS\n",
    "    metrics['val_accuracy'] = [accuracy_score(y_true, y_pred)]\n",
    "    metrics['val_precision'] = [precision_score(y_true, y_pred)]\n",
    "    metrics['val_sensitivity'] = [recall_score(y_true, y_pred)]\n",
    "    metrics['val_specificity'] = [specificity(tn,fp)]\n",
    "    metrics['val_f1_score'] =[f1_score(y_true, y_pred)]\n",
    "    metrics['val_npv'] = [npv(tn, fn)]\n",
    "    metrics['val_mcc'] = [mcc(tp, tn, fp, fn)]\n",
    "    metrics['val_auc'] = [auc_val]\n",
    "    metrics['val_TP'] = [tp]\n",
    "    metrics['val_TN'] = [tn]\n",
    "    metrics['val_FP'] = [fp]\n",
    "    metrics['val_FN'] = [fn]\n",
    "    metrics['val_runtime'] = [runtimeTest]\n",
    "\n",
    "\n",
    "\n",
    "    print(bcolors.FAIL + 'ACC: %.2f' %(100*metrics['val_accuracy'][0]) + ' AUC: %.2f' %(100*metrics['val_auc'][0]) + bcolors.ENDC)\n",
    "\n",
    "    if os.path.exists(os.path.join(save_metrics_path, methodName + '.csv')):\n",
    "        metrics.to_csv(os.path.join(save_metrics_path, methodName + '.csv'), sep=',', mode='a', index=False, header=False)\n",
    "    else:\n",
    "        metrics.to_csv(os.path.join(save_metrics_path, methodName + '.csv'), sep=',', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06204914  0.9895307  15.90607122 ...  0.99509313  0.99391009\n",
      "   0.89674086]\n",
      " [ 0.06225552  0.9960983  15.90608004 ...  0.99822709  0.99741641\n",
      "   0.83978684]\n",
      " [ 0.06220966  0.9946368  15.90608164 ...  0.99739309  0.99667822\n",
      "   0.86922134]\n",
      " ...\n",
      " [ 0.06227845  0.99682915 15.90608595 ...  0.99833793  0.99784699\n",
      "   0.8409174 ]\n",
      " [ 0.06231667  0.99804825 15.9060863  ...  0.99887713  0.99852367\n",
      "   0.80066593]\n",
      " [ 0.06220582  0.99451476 15.90608764 ...  0.99701703  0.99655518\n",
      "   0.82892093]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_dataset(csv_path: str) -> tuple:\n",
    "    features = []\n",
    "    \"\"\"_summary_\n",
    "    # feature header histogram_Mean,histogram_Standard Deviation,histogram_Energy,histogram_Skewness,histogram_Entropy,histogram_Smoothness,histogram_Kurtosis,glcm_Contrast,glcm_Dissimilarity,glcm_Homogeneity,glcm_Energy,glcm_Correlation,image\n",
    "    # image 6-66-53-0.bmp 0 is the label\n",
    "\n",
    "    For each features file, the last column is the image name with the label in your name\n",
    "    \"\"\"\n",
    "\n",
    "    pd_data = pd.read_csv(csv_path)\n",
    "\n",
    "    # Get the features\n",
    "    features = pd_data.iloc[:, :-1].values\n",
    " \n",
    "    # Get the label  (last column), which is the image name with the label in your name, apply lambda to get only the label\n",
    "    labels = pd_data.iloc[:, -1].apply(lambda x: int(x.split('-')[-1].split('.')[0])).values\n",
    "\n",
    "    # remove column image from features\n",
    "\n",
    "    features = np.delete(features, 0, axis=1)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "all_datasets = {}\n",
    "for dataset_name, path in paths_datasets.items():\n",
    "    X, y = load_dataset(path)\n",
    "    all_datasets[dataset_name] = (X, y)\n",
    "\n",
    "\n",
    "splited_datasets = {}\n",
    "for dataset_name, (X, y) in all_datasets.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    splited_datasets[dataset_name] = (X_train, X_test, y_train, y_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GradientBoosting on Dataset01_5.0\n",
      "X_train shape: (4801, 11) y_train shape: (4801,)\n",
      "X_test shape: (1201, 11) y_test shape: (1201,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "10 fits failed out of a total of 20.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of GradientBoostingClassifier must be a str among {'exponential', 'log_loss'}. Got 'deviance' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of GradientBoostingClassifier must be a str among {'log_loss', 'exponential'}. Got 'deviance' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.7755102  0.83673469\n",
      " 0.83673469 0.85714286 0.87755102 0.75510204 0.87755102 0.87755102\n",
      " 0.87755102 0.87755102]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.8318872451782227\n",
      "Test time: 0.0031549930572509766\n",
      "\u001b[91mACC: 83.51 AUC: 90.63\u001b[0m\n",
      "Finished GradientBoosting on Dataset01_5.0\n",
      "Memory cleaned\n",
      "Training GradientBoosting on Dataset01_10.0\n",
      "X_train shape: (4801, 11) y_train shape: (4801,)\n",
      "X_test shape: (1201, 11) y_test shape: (1201,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "10 fits failed out of a total of 20.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "8 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of GradientBoostingClassifier must be a str among {'log_loss', 'exponential'}. Got 'deviance' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of GradientBoostingClassifier must be a str among {'exponential', 'log_loss'}. Got 'deviance' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/williancarddd/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.81632653 0.83673469\n",
      " 0.83673469 0.85714286 0.83673469 0.83673469 0.83673469 0.83673469\n",
      " 0.83673469 0.85714286]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.2021982669830322\n",
      "Test time: 0.003930568695068359\n",
      "\u001b[91mACC: 87.26 AUC: 93.56\u001b[0m\n",
      "Finished GradientBoosting on Dataset01_10.0\n",
      "Memory cleaned\n",
      "Training GradientBoosting on Dataset01_15.0\n",
      "X_train shape: (4801, 11) y_train shape: (4801,)\n",
      "X_test shape: (1201, 11) y_test shape: (1201,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory cleaned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[91], line 10\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m y_test shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_test\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ml_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m runtimeTrain \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mruntimeTrain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[69], line 119\u001b[0m, in \u001b[0;36mtrain_ml_algorithm\u001b[0;34m(X_train, y_train, methodName)\u001b[0m\n\u001b[1;32m    115\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (search \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 119\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1572\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    961\u001b[0m         )\n\u001b[1;32m    962\u001b[0m     )\n\u001b[0;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    for network in methodsNames:\n",
    "        for dataset_name, (X, y) in all_datasets.items():\n",
    "            X_train, X_test, y_train, y_test = splited_datasets[dataset_name]\n",
    "            path_metrics, path_csvs, path_test = make_results_folders(path_results, dataset_name, network)\n",
    "            print(f\"Training {network} on {dataset_name}\")\n",
    "            print(f\"X_train shape: {X_train.shape} y_train shape: {y_train.shape}\")\n",
    "            print(f\"X_test shape: {X_test.shape} y_test shape: {y_test.shape}\")\n",
    "            start = time.time()\n",
    "            results = train_ml_algorithm(X_train, y_train, network)\n",
    "            runtimeTrain = time.time() - start\n",
    "            print(f\"Training time: {runtimeTrain}\")\n",
    "            start = time.time()\n",
    "            y_pred = results.predict(X_test)\n",
    "            y_scores = results.predict_proba(X_test)[:, 1]\n",
    "            runtimeTest = time.time() - start\n",
    "            print(f\"Test time: {runtimeTest}\")\n",
    "            calculateMeasures(dataset_name, results.predict(X_train), y_train, results.predict_proba(X_train)[:, 1], y_pred, y_test, y_scores, 'test', network + \"f\", 0.5, path_metrics, runtimeTrain, runtimeTest)\n",
    "            print(f\"Finished {network} on {dataset_name}\")\n",
    "            del results\n",
    "            gc.collect()\n",
    "            print(\"Memory cleaned\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
